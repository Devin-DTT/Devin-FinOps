From 514c7f38fbdb33c74e1c3b736fa9cd6aeedd2af0 Mon Sep 17 00:00:00 2001
From: Devin AI <158243242+devin-ai-integration[bot]@users.noreply.github.com>
Date: Wed, 22 Oct 2025 10:28:08 +0000
Subject: [PATCH] P5: Implement Cognition API data adapter and centralized
 logging

- Created data_adapter.py to fetch data from Cognition API
  - Supports pagination with skip/limit/has_more
  - Bearer token authentication via DEVIN_ENTERPRISE_API_KEY
  - Robust error handling for 401/403 and connection errors
  - Writes output to raw_usage_data.json

- Implemented centralized logging system
  - finops_pipeline.log for detailed logs (all levels)
  - Console output for ERROR/CRITICAL only
  - Consistent format across all modules

- Added logging to metrics_calculator.py
  - Logs data loading, metric calculation start/end

- Updated generate_report.py orchestration
  - Calls data_adapter to fetch fresh data before processing
  - Uses centralized logging configuration
  - Maintains backward compatibility with existing data files

Co-Authored-By: gtorreshuamantica@deloitte.es <gtorreshuamantica@deloitte.es>
---
 data_adapter.py       | 197 ++++++++++++++++++++++++++++++++++++++++++
 generate_report.py    |  14 ++-
 metrics_calculator.py |  12 ++-
 3 files changed, 218 insertions(+), 5 deletions(-)
 create mode 100644 data_adapter.py

diff --git a/data_adapter.py b/data_adapter.py
new file mode 100644
index 0000000..82948e7
--- /dev/null
+++ b/data_adapter.py
@@ -0,0 +1,197 @@
+"""
+Data Adapter Module for Cognition API Integration
+Fetches usage data from the Cognition API and writes to raw_usage_data.json
+"""
+
+import os
+import json
+import logging
+import requests
+from typing import List, Dict, Any, Optional
+
+
+logger = logging.getLogger(__name__)
+
+
+def setup_logging():
+    """Configure centralized logging for the FinOps pipeline."""
+    root_logger = logging.getLogger()
+    root_logger.setLevel(logging.DEBUG)
+    
+    root_logger.handlers.clear()
+    
+    file_handler = logging.FileHandler('finops_pipeline.log', mode='a')
+    file_handler.setLevel(logging.DEBUG)
+    file_formatter = logging.Formatter(
+        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+    )
+    file_handler.setFormatter(file_formatter)
+    
+    console_handler = logging.StreamHandler()
+    console_handler.setLevel(logging.ERROR)
+    console_formatter = logging.Formatter(
+        '%(asctime)s - %(levelname)s - %(message)s'
+    )
+    console_handler.setFormatter(console_formatter)
+    
+    root_logger.addHandler(file_handler)
+    root_logger.addHandler(console_handler)
+
+
+def fetch_cognition_data(
+    base_url: str = "https://api.devin.ai/v2/enterprise",
+    endpoint: str = "/consumption/daily-consumption",
+    api_key: Optional[str] = None,
+    page_size: int = 100
+) -> List[Dict[str, Any]]:
+    """
+    Fetch usage data from Cognition API with pagination.
+    
+    Args:
+        base_url: Base URL for the Cognition API
+        endpoint: API endpoint path
+        api_key: API key for authentication (from DEVIN_ENTERPRISE_API_KEY)
+        page_size: Number of records per page
+    
+    Returns:
+        List of usage data records
+    
+    Raises:
+        requests.exceptions.RequestException: For connection/HTTP errors
+        ValueError: For authentication errors
+    """
+    logger.info("Starting data fetch from Cognition API")
+    logger.info(f"API Base URL: {base_url}")
+    logger.info(f"Endpoint: {endpoint}")
+    
+    if api_key is None:
+        api_key = os.getenv('DEVIN_ENTERPRISE_API_KEY')
+        if not api_key:
+            error_msg = "DEVIN_ENTERPRISE_API_KEY environment variable not set"
+            logger.error(error_msg)
+            raise ValueError(error_msg)
+    
+    full_url = f"{base_url.rstrip('/')}{endpoint}"
+    logger.info(f"Full API URL: {full_url}")
+    
+    headers = {
+        'Authorization': f'Bearer {api_key}',
+        'Content-Type': 'application/json'
+    }
+    
+    all_data = []
+    skip = 0
+    has_more = True
+    page_count = 0
+    
+    try:
+        while has_more:
+            page_count += 1
+            params = {
+                'skip': skip,
+                'limit': page_size
+            }
+            
+            logger.info(f"Fetching page {page_count} (skip={skip}, limit={page_size})")
+            
+            try:
+                response = requests.get(
+                    full_url,
+                    headers=headers,
+                    params=params,
+                    timeout=30
+                )
+                
+                if response.status_code == 401:
+                    error_msg = "Authentication failed (401): Invalid API key"
+                    logger.error(error_msg)
+                    raise ValueError(error_msg)
+                
+                if response.status_code == 403:
+                    error_msg = "Authorization failed (403): Access denied"
+                    logger.error(error_msg)
+                    raise ValueError(error_msg)
+                
+                response.raise_for_status()
+                
+                response_data = response.json()
+                
+                page_data = response_data.get('data', [])
+                all_data.extend(page_data)
+                
+                logger.info(f"Page {page_count} fetched: {len(page_data)} records")
+                
+                has_more = response_data.get('has_more', False)
+                skip += page_size
+                
+            except requests.exceptions.Timeout:
+                error_msg = f"Request timeout on page {page_count}"
+                logger.error(error_msg)
+                raise requests.exceptions.RequestException(error_msg)
+            
+            except requests.exceptions.ConnectionError as e:
+                error_msg = f"Connection error on page {page_count}: {e}"
+                logger.error(error_msg)
+                raise
+            
+            except requests.exceptions.HTTPError as e:
+                error_msg = f"HTTP error on page {page_count}: {e}"
+                logger.error(error_msg)
+                raise
+        
+        logger.info(f"Data fetch complete: {len(all_data)} total records from {page_count} pages")
+        return all_data
+    
+    except Exception as e:
+        logger.error(f"Failed to fetch data from Cognition API: {e}", exc_info=True)
+        raise
+
+
+def write_raw_data(data: List[Dict[str, Any]], output_file: str = 'raw_usage_data.json') -> None:
+    """
+    Write fetched data to JSON file.
+    
+    Args:
+        data: List of usage data records
+        output_file: Output file path
+    """
+    logger.info(f"Writing {len(data)} records to {output_file}")
+    
+    try:
+        with open(output_file, 'w') as f:
+            json.dump(data, f, indent=2)
+        
+        logger.info(f"Successfully wrote data to {output_file}")
+    
+    except Exception as e:
+        logger.error(f"Failed to write data to {output_file}: {e}", exc_info=True)
+        raise
+
+
+def main():
+    """Main execution function for data adapter."""
+    setup_logging()
+    
+    logger.info("=" * 60)
+    logger.info("FinOps Data Adapter - Cognition API Integration")
+    logger.info("=" * 60)
+    
+    try:
+        data = fetch_cognition_data()
+        
+        write_raw_data(data)
+        
+        logger.info("=" * 60)
+        logger.info("Data adapter completed successfully")
+        logger.info("=" * 60)
+        
+        return True
+    
+    except Exception as e:
+        logger.error(f"Data adapter failed: {e}", exc_info=True)
+        return False
+
+
+if __name__ == '__main__':
+    success = main()
+    exit(0 if success else 1)
diff --git a/generate_report.py b/generate_report.py
index 1388ad1..61878db 100644
--- a/generate_report.py
+++ b/generate_report.py
@@ -15,11 +15,8 @@ from typing import Dict, Any, List
 from collections import defaultdict
 from metrics_calculator import MetricsCalculator
 from config import MetricsConfig
+import data_adapter
 
-logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(levelname)s - %(message)s'
-)
 logger = logging.getLogger(__name__)
 
 
@@ -158,10 +155,19 @@ def flatten_metrics(metrics_dict: Dict[str, Any], config: MetricsConfig) -> List
 
 def main():
     """Main execution function for report generation."""
+    data_adapter.setup_logging()
+    
     logger.info("=" * 60)
     logger.info("FinOps Metrics Report Generator - Phase 3")
     logger.info("=" * 60)
     
+    logger.info("Fetching data from Cognition API...")
+    try:
+        data_adapter.main()
+    except Exception as e:
+        logger.error(f"Failed to fetch data from API: {e}")
+        logger.info("Attempting to use existing raw_usage_data.json if available")
+    
     raw_data_file = 'raw_usage_data.json'
     output_file = 'finops_metrics_report.csv'
     temp_transformed_file = 'transformed_usage_data.json'
diff --git a/metrics_calculator.py b/metrics_calculator.py
index 71314ce..cca59b6 100644
--- a/metrics_calculator.py
+++ b/metrics_calculator.py
@@ -4,10 +4,13 @@ This module processes raw usage data and calculates 20 foundational metrics.
 """
 
 import json
+import logging
 from typing import Dict, List, Any
 from collections import defaultdict
 from config import MetricsConfig
 
+logger = logging.getLogger(__name__)
+
 
 class MetricsCalculator:
     """
@@ -39,11 +42,13 @@ class MetricsCalculator:
             FileNotFoundError: If the file doesn't exist
             json.JSONDecodeError: If the file is not valid JSON
         """
+        logger.info(f"Loading data from {json_file_path}")
         with open(json_file_path, 'r') as f:
             self.data = json.load(f)
 
         self.sessions = self.data.get('sessions', [])
         self.users = self.data.get('user_details', [])
+        logger.info(f"Data loaded: {len(self.sessions)} sessions, {len(self.users)} users")
 
     def calculate_total_monthly_cost(self) -> float:
         """
@@ -341,7 +346,9 @@ class MetricsCalculator:
         Returns:
             Dictionary containing all calculated metrics
         """
-        return {
+        logger.info("Starting calculation of all metrics")
+        
+        metrics_result = {
             'config': self.config.to_dict(),
             'reporting_period': self.data.get('reporting_period', {}),
             'metrics': {
@@ -367,6 +374,9 @@ class MetricsCalculator:
                 '20_efficiency_ratio': self.calculate_efficiency_ratio()
             }
         }
+        
+        logger.info("Metrics calculation complete")
+        return metrics_result
 
 
 def main():
-- 
2.34.1

